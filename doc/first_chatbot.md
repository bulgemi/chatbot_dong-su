# 처음 배우는 딥러닝 챗봇

> 조경래 지음, 한빛미디어 

## 토크나이징

> 주어진 문장에서 토근 단위로 정보를 나누는 작업<br/>
> 문장 형태의 데이터를 처리하기 위해 제일 처음 수행하는 기본적인 작업<br/>
> 주로 텍스트 전처리 과정에 사용

* 자연어 의미를 분석해 컴퓨터가 처리할 수 있도록 하는 일을 자연어 처리(Natural Language Processing, NLP)라고 함.
* 토큰(token): 문장에서 가장 기본이 되는 단어

### KoNLPy

> 한국어 자연어 처리를 위한 파이썬 라이브러리<br/>
> 형태소 분석기는 사전 관리 편리하고 성능/속도 준수한 Komoran 사용 

* 토큰 단위: 형태소
  * 형태소는 언어학에서 사용되는 용어로 일정한 의미가 있는 가장 작은 말의 단위 
* 형태소 분석기: 언어의 특성에 따라 형태소 분석할 수 있는 도구 
  * 문장에서 형태소뿐만 아니라 어근, 접두사/접미사, 품사 등 다양한 언어적 속성 파악 
  * 품사 태깅: 단어를 의미나 형식, 기능에 따라 분류하는 작업

#### Kkma

> 서울대학교 IDS(Intelligent Data Systems) 연구실에서 자연어 처리를 위해 개발한 한국어 형태소 분석기<br/>
> 속도가 느리더라도 정확한 품사 정보가 필요할 때 사용

* Kkma 모듈 함수
  * morphs(phrase): 인자로 입력한 문장을 형태소 단위로 토크나이징
  * nouns(phrase): 인자로 입력한 문장에서 품사가 명사인 토큰만 추출
  * pos(phrase, flatten=True): 인자로 입력한 문장에서 형태소를 추출한 뒤 품사 태깅
  * sentences(phrase): 인자로 입력한 여러 문장을 분리해주는 역할

#### Komoran(Korean Morphological ANalyzer)

> Shineware에서 자바로 개발한 한국어 형태소 분석기<br/>
> Komoran은 Kkma보더 형태소를 빠르게 분석하며 다양한 품사 테그 지원

* Komoran 모듈 함수
  * morphs(phrase): 인자로 입력한 문장을 형태소 단위로 토크나이징
  * nouns(phrase): 인자로 입력한 문장에서 품사가 명사인 토큰만 추출
  * pos(phrase, flatten=True): 인자로 입력한 문장에서 형태소를 추출한 뒤 품사 태깅
* Komoran 품사 태그 표
  * NNG: 일반명서
  * JKS: 주격조사
  * JKB: 부사격 조사
  * VV: 동사
  * EF: 종결어미
  * SF: 마침표, 물음표, 느낌표

#### Okt(Open-source Korean Text Processor)

> 트위터에서 개발한 한국어 처리기<br/>
> 분석되는 품사 정보는 작지만 분석 속도는 제일 빠름<br/>
> normalize() 함수를 지원해 오타 섞인 문장 정규화시 좋음

* Okt 모듈 함수
  * morphs(phrase): 인자로 입력한 문장을 형태소 단위로 토크나이징
  * nouns(phrase): 인자로 입력한 문장에서 품사가 명사인 토큰만 추출
  * pos(phrase, stem=False, join=False): 인자로 입력한 문장에서 형태소를 추출한 뒤 품사 태깅
  * normalize(phrase): 입력한 문장 정규화
  * phrase(phrase): 입력한 문장에서 어구 추출  

### 사용자 사전 구축

> 새롭게 생겨나는 단어나 문장은 형태소 분석기에서 인식이 안 되는 경우가 많고 이를 해결하기 위해 대부분의 형태소 분석기들은 
> 사용자 사전을 추가할 수 있도록 사용자 사전 제공 

* user_dic.tsv 파일로 저장
  * 단어와 품사 `Tab`으로 구분 
  ```tsv
  # [단어] Tab [품사]
  엔엘피 Tab NNG
  DS TF Tab NNG
  ```
### 형태소 분석기 비교

| 형태소 분석기 | 장점 | 단점 |
| --- | --- | --- |
| Kkma | 분석 품질 좋음, 지원 품사 태그 많음 | 분석 속도 느림, 사용자 사전 불완전 동작 |
| Komoran | 자소 분리된 문작과 오탈자에 강함, 사용자 사전 관리 용이 | 적당한 분석 품질과 속도 |
| Okt | 매우 빠른 처리 속도, 정규화 기능 지원 | 사용자 사전 관리 어려움, 분석 일관성 부족 |

## 임베딩

> 단어나 문장을 수치화해 벡터 공간으로 표현하는 과정<br/>
> 임베딩 품질이 좋다면 단순한 모델로도 훌륭한 결과를 얻을 수 있다.

* 임베딩은 말뭉치의 의미에 따라 벡터화하기 때문에 문법적인 정보가 포함되어 있다.
* 임베딩 기법
  * 문장 임베딩
    * 문장 전체를 벡터로 표현하는 방법
    * 전체 문장의 흐름을 파악해 벡터로 변환하기 때문에 문맥적 의미를 지니는 장점 
    * 장점: 단어 임베딩에 비해 품질이 좋으며, 상용 시스템에 많이 사용 
    * 단점: 많은 문장 데이터 필요, 학습 비용 큼.
  * 단어 임베딩 
    * 개별 단어를 벡터로 표현하는 방법
    * 장점: 학습 방법 간단, 실무 많이 사용 
    * 단점: 동음이의어 구분 안 됨 

### 단어 임베딩

> 단어 임베딩은 말뭉치에서 각각의 단어를 벡터로 변환하는 기법 

#### 원-핫 인코딩(one-hot encoding)

> 단어를 숫자 벡터로 변환하는 가장 기본적인 방법

* 요소들 중 단 하나의 값만 1이고 나머지 요소값은 0인 인코딩
* 원-핫 인코딩으로 나온 결과를 원-핫 벡터
* 원-핫 인코딩을 하기 위해서 단어 집합이라 불리는 사전을 만들어야 함
  * 사전: 말뭉치에서 나오는 서로 다른 단어의 집합 
* 장점: 간단한 구현 방법에 비해 좋은 성능 제공
* 단점: 단어의 의미, 유사한 단어 관계 정보 없음, 단어 사전 크기가 커지면 메모리 낭비와 복잡도 증가(비효율적)

#### 희소 표현과 분산 표현

* 희소 표현
  * 원-핫 인코딩은 희소 벡터(희소 행렬)이고 이런 희소 벡터 표현 방식을 희소 벡터라고 함.
* 분산 표현 
  * 자연어 처리를 잘하기 위해서 기본 토큰이 되는 단어의 의미와 주변 단어 간의 관계가 단어 임베딩에 표현되어야 함.
  * 각 단어 간의 유사성을 잘 표현하면서 벡터 공간을 절약할 수 있는 방법
  * 한 단어의 정보가 여러 차원에 분산되어 표현
  * 분산 표현 방식은 우리가 원하는 차원에 데이터를 최대한 밀집시킬수 있어 밀집 표현(dense representation)이라 부르기도 하며, 밀집 표현으로 만들어진 벡터를 밀집 벡터(dense vector)라고 함.   
  * 장점
    1. 임베팅 벡터의 차원을 데이터 손실을 최소화하면서 압축
    2. 단어의 의미, 주변 단어간의 관계 등 많은 정보가 내포되어 있어 일반화 능력 뛰어남.

#### Word2Vec

## 텍스트 유사도

## 챗봇 엔진에 필요한 딥러닝 모델

## 챗봇 학습툴 만들기

## 챗봇 엔진 만들기

## 챗봇 API 만들기
