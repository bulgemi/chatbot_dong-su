# 처음 배우는 딥러닝 챗봇

> 조경래 지음, 한빛미디어 

## 토크나이징

> 주어진 문장에서 토근 단위로 정보를 나누는 작업<br/>
> 문장 형태의 데이터를 처리하기 위해 제일 처음 수행하는 기본적인 작업<br/>
> 주로 텍스트 전처리 과정에 사용

* 자연어 의미를 분석해 컴퓨터가 처리할 수 있도록 하는 일을 자연어 처리(Natural Language Processing, NLP)라고 함.
* 토큰(token): 문장에서 가장 기본이 되는 단어

### KoNLPy

> 한국어 자연어 처리를 위한 파이썬 라이브러리<br/>
> 형태소 분석기는 사전 관리 편리하고 성능/속도 준수한 Komoran 사용 

* 토큰 단위: 형태소
  * 형태소는 언어학에서 사용되는 용어로 일정한 의미가 있는 가장 작은 말의 단위 
* 형태소 분석기: 언어의 특성에 따라 형태소 분석할 수 있는 도구 
  * 문장에서 형태소뿐만 아니라 어근, 접두사/접미사, 품사 등 다양한 언어적 속성 파악 
  * 품사 태깅: 단어를 의미나 형식, 기능에 따라 분류하는 작업

#### Kkma

> 서울대학교 IDS(Intelligent Data Systems) 연구실에서 자연어 처리를 위해 개발한 한국어 형태소 분석기<br/>
> 속도가 느리더라도 정확한 품사 정보가 필요할 때 사용

* Kkma 모듈 함수
  * morphs(phrase): 인자로 입력한 문장을 형태소 단위로 토크나이징
  * nouns(phrase): 인자로 입력한 문장에서 품사가 명사인 토큰만 추출
  * pos(phrase, flatten=True): 인자로 입력한 문장에서 형태소를 추출한 뒤 품사 태깅
  * sentences(phrase): 인자로 입력한 여러 문장을 분리해주는 역할

#### Komoran(Korean Morphological ANalyzer)

> Shineware에서 자바로 개발한 한국어 형태소 분석기<br/>
> Komoran은 Kkma보더 형태소를 빠르게 분석하며 다양한 품사 테그 지원

* Komoran 모듈 함수
  * morphs(phrase): 인자로 입력한 문장을 형태소 단위로 토크나이징
  * nouns(phrase): 인자로 입력한 문장에서 품사가 명사인 토큰만 추출
  * pos(phrase, flatten=True): 인자로 입력한 문장에서 형태소를 추출한 뒤 품사 태깅
* Komoran 품사 태그 표
  * NNG: 일반명서
  * JKS: 주격조사
  * JKB: 부사격 조사
  * VV: 동사
  * EF: 종결어미
  * SF: 마침표, 물음표, 느낌표

#### Okt(Open-source Korean Text Processor)

> 트위터에서 개발한 한국어 처리기<br/>
> 분석되는 품사 정보는 작지만 분석 속도는 제일 빠름<br/>
> normalize() 함수를 지원해 오타 섞인 문장 정규화시 좋음

* Okt 모듈 함수
  * morphs(phrase): 인자로 입력한 문장을 형태소 단위로 토크나이징
  * nouns(phrase): 인자로 입력한 문장에서 품사가 명사인 토큰만 추출
  * pos(phrase, stem=False, join=False): 인자로 입력한 문장에서 형태소를 추출한 뒤 품사 태깅
  * normalize(phrase): 입력한 문장 정규화
  * phrase(phrase): 입력한 문장에서 어구 추출  

### 사용자 사전 구축

> 새롭게 생겨나는 단어나 문장은 형태소 분석기에서 인식이 안 되는 경우가 많고 이를 해결하기 위해 대부분의 형태소 분석기들은 
> 사용자 사전을 추가할 수 있도록 사용자 사전 제공 

* user_dic.tsv 파일로 저장
  * 단어와 품사 `Tab`으로 구분 
  ```tsv
  # [단어] Tab [품사]
  엔엘피 Tab NNG
  DS TF Tab NNG
  ```
### 형태소 분석기 비교

| 형태소 분석기 | 장점 | 단점 |
| --- | --- | --- |
| Kkma | 분석 품질 좋음, 지원 품사 태그 많음 | 분석 속도 느림, 사용자 사전 불완전 동작 |
| Komoran | 자소 분리된 문작과 오탈자에 강함, 사용자 사전 관리 용이 | 적당한 분석 품질과 속도 |
| Okt | 매우 빠른 처리 속도, 정규화 기능 지원 | 사용자 사전 관리 어려움, 분석 일관성 부족 |

## 임베딩

> 단어나 문장을 수치화해 벡터 공간으로 표현하는 과정<br/>
> 임베딩 품질이 좋다면 단순한 모델로도 훌륭한 결과를 얻을 수 있다.

* 임베딩은 말뭉치의 의미에 따라 벡터화하기 때문에 문법적인 정보가 포함되어 있다.
* 임베딩 기법
  * 문장 임베딩
    * 문장 전체를 벡터로 표현하는 방법
    * 전체 문장의 흐름을 파악해 벡터로 변환하기 때문에 문맥적 의미를 지니는 장점 
    * 장점: 단어 임베딩에 비해 품질이 좋으며, 상용 시스템에 많이 사용 
    * 단점: 많은 문장 데이터 필요, 학습 비용 큼.
  * 단어 임베딩 
    * 개별 단어를 벡터로 표현하는 방법
    * 장점: 학습 방법 간단, 실무 많이 사용 
    * 단점: 동음이의어 구분 안 됨 

### 단어 임베딩

> 단어 임베딩은 말뭉치에서 각각의 단어를 벡터로 변환하는 기법 

#### 원-핫 인코딩(one-hot encoding)

> 단어를 숫자 벡터로 변환하는 가장 기본적인 방법

* 요소들 중 단 하나의 값만 1이고 나머지 요소값은 0인 인코딩
* 원-핫 인코딩으로 나온 결과를 원-핫 벡터
* 원-핫 인코딩을 하기 위해서 단어 집합이라 불리는 사전을 만들어야 함
  * 사전: 말뭉치에서 나오는 서로 다른 단어의 집합 
* 장점: 간단한 구현 방법에 비해 좋은 성능 제공
* 단점: 단어의 의미, 유사한 단어 관계 정보 없음, 단어 사전 크기가 커지면 메모리 낭비와 복잡도 증가(비효율적)

#### 희소 표현과 분산 표현

* 희소 표현
  * 원-핫 인코딩은 희소 벡터(희소 행렬)이고 이런 희소 벡터 표현 방식을 희소 벡터라고 함.
* 분산 표현 
  * 자연어 처리를 잘하기 위해서 기본 토큰이 되는 단어의 의미와 주변 단어 간의 관계가 단어 임베딩에 표현되어야 함.
  * 각 단어 간의 유사성을 잘 표현하면서 벡터 공간을 절약할 수 있는 방법
  * 한 단어의 정보가 여러 차원에 분산되어 표현
  * 분산 표현 방식은 우리가 원하는 차원에 데이터를 최대한 밀집시킬수 있어 밀집 표현(dense representation)이라 부르기도 하며, 밀집 표현으로 만들어진 벡터를 밀집 벡터(dense vector)라고 함.   
  * 장점
    1. 임베팅 벡터의 차원을 데이터 손실을 최소화하면서 압축
    2. 단어의 의미, 주변 단어간의 관계 등 많은 정보가 내포되어 있어 일반화 능력 뛰어남.

#### Word2Vec

> 2013년 구글에서 발표했으며 가장 많이 사용하고 있는 단어 임베딩 모델<br/>
> 계산량을 획기적으로 줄여 빠른 학습 가능<br/>

* Word2Vec 모델은 CBOW(Continuous bag-of-words, 이하 CBOW)와 skip-gram 두 가지 모델 제안
  * CBOW 모델: 주변 단어들을 이용해 타깃 단어를 예측하는 신경망 모델
  * skip-gram 모델: CBOW 모델과 반대로 하나의 타깃 단어를 이용해 주변 단어들을 예측하는 신경망 모델 
* Gensim 패키지 사용: 토픽 모델링과 자연어 처리를 위한 라이브러리
  * 사용법이 간단하고 임베딩 품질이 나쁘지 않아 많이 사용 

## 텍스트 유사도

> 문장을 하나의 벡터로 묶어서 문장간 유사도 계산

### n-gram 유사도

> n-gram은 주어진 문장에서 n개의 연속적인 단어 시퀀스(단어 나열)를 의미<br/>
> n개의 단어를 토큰으로 사용, 이웃한 단어의 출현 횟수를 통계적으로 표현해 텍스트의 유사도를 계산하는 방법<br/>

* 장점: 손쉬운 구현 방식에 비해 학습 말뭉치 품질만 좋다면 괜찮은 성능 제공
* n에 따른 n-gram
  1. n이 1인 경우
     * 1-gram
     * 유니그램
  2. n이 2인 경우
     * 2-gram
     * 바이그램
  3. n이 3인 경우
     * 3-gram
     * 트라이그램
  4. n이 4 이상
     * 숫자만 앞쪽에 붙임
* n-gram 유사도 계산 흐름
  1. 문장을 n-gram으로 토큰을 분리한 후 단어 문서 행렬(Term-Document Matrix, TDM) 생성
  2. 두 문장을 서로 비교해여 동일한 단어의 출현 빈도를 확률로 계산 
* 보통 1~5 사이의 값 많이 사용

### 코사인 유사도

> 단어나 문장을 벡터로 표현할 수 있다면 벡터 간 거리나 각도를 이용해 유사성 파악<br/>
> 두 벡터 간 코사인 각도를 이용해 유사도를 측정하는 방법

* 벡터의 크기와 상관없이 결과가 안정적임
* 다양한 차원에서 적용 가능해 실무에서 많이 사용

## 챗봇 엔진에 필요한 딥러닝 모델

* 케라스: 신경망 모델을 구추할 수 있는 고수준 API 라이브러리

### 인공 신경망(Artificial Neural Network)

> 두뇌의 신경세포인 뉴런을 수학적으로 모방한 모델 

<img src="https://render.githubusercontent.com/render/math?math=y = (w_0x_0 %2B w_1x_1 %2B w_2x_2) %2B b">

* 실제 뉴런은 입력된 신호가 특

### 문장 분류를 위한 CNN 모델

#### CNN 모델 개념

#### 챗봇 문답 데이터 감정 분류 모델 구현

### 개체명 인식을 위한 양방향 LSTM 모델

#### RNN

#### LSTM

#### 양방향 LSTM

#### 개체명 인식

## 챗봇 학습툴 만들기

## 챗봇 엔진 만들기

## 챗봇 API 만들기
