# 처음 배우는 딥러닝 챗봇

> 조경래 지음, 한빛미디어 

## 토크나이징

> 주어진 문장에서 토근 단위로 정보를 나누는 작업<br/>
> 문장 형태의 데이터를 처리하기 위해 제일 처음 수행하는 기본적인 작업<br/>
> 주로 텍스트 전처리 과정에 사용

* 자연어 의미를 분석해 컴퓨터가 처리할 수 있도록 하는 일을 자연어 처리(Natural Language Processing, NLP)라고 함.
* 토큰(token): 문장에서 가장 기본이 되는 단어

### KoNLPy

> 한국어 자연어 처리를 위한 파이썬 라이브러리<br/>
> 형태소 분석기는 사전 관리 편리하고 성능/속도 준수한 Komoran 사용 

* 토큰 단위: 형태소
  * 형태소는 언어학에서 사용되는 용어로 일정한 의미가 있는 가장 작은 말의 단위 
* 형태소 분석기: 언어의 특성에 따라 형태소 분석할 수 있는 도구 
  * 문장에서 형태소뿐만 아니라 어근, 접두사/접미사, 품사 등 다양한 언어적 속성 파악 
  * 품사 태깅: 단어를 의미나 형식, 기능에 따라 분류하는 작업

#### Kkma

> 서울대학교 IDS(Intelligent Data Systems) 연구실에서 자연어 처리를 위해 개발한 한국어 형태소 분석기<br/>
> 속도가 느리더라도 정확한 품사 정보가 필요할 때 사용

* Kkma 모듈 함수
  * morphs(phrase): 인자로 입력한 문장을 형태소 단위로 토크나이징
  * nouns(phrase): 인자로 입력한 문장에서 품사가 명사인 토큰만 추출
  * pos(phrase, flatten=True): 인자로 입력한 문장에서 형태소를 추출한 뒤 품사 태깅
  * sentences(phrase): 인자로 입력한 여러 문장을 분리해주는 역할

#### Komoran(Korean Morphological ANalyzer)

> Shineware에서 자바로 개발한 한국어 형태소 분석기<br/>
> Komoran은 Kkma보더 형태소를 빠르게 분석하며 다양한 품사 테그 지원

* Komoran 모듈 함수
  * morphs(phrase): 인자로 입력한 문장을 형태소 단위로 토크나이징
  * nouns(phrase): 인자로 입력한 문장에서 품사가 명사인 토큰만 추출
  * pos(phrase, flatten=True): 인자로 입력한 문장에서 형태소를 추출한 뒤 품사 태깅
* Komoran 품사 태그 표
  * NNG: 일반명서
  * JKS: 주격조사
  * JKB: 부사격 조사
  * VV: 동사
  * EF: 종결어미
  * SF: 마침표, 물음표, 느낌표

#### Okt(Open-source Korean Text Processor)

> 트위터에서 개발한 한국어 처리기<br/>
> 분석되는 품사 정보는 작지만 분석 속도는 제일 빠름<br/>
> normalize() 함수를 지원해 오타 섞인 문장 정규화시 좋음

* Okt 모듈 함수
  * morphs(phrase): 인자로 입력한 문장을 형태소 단위로 토크나이징
  * nouns(phrase): 인자로 입력한 문장에서 품사가 명사인 토큰만 추출
  * pos(phrase, stem=False, join=False): 인자로 입력한 문장에서 형태소를 추출한 뒤 품사 태깅
  * normalize(phrase): 입력한 문장 정규화
  * phrase(phrase): 입력한 문장에서 어구 추출  

### 사용자 사전 구축

> 새롭게 생겨나는 단어나 문장은 형태소 분석기에서 인식이 안 되는 경우가 많고 이를 해결하기 위해 대부분의 형태소 분석기들은 
> 사용자 사전을 추가할 수 있도록 사용자 사전 제공 

* user_dic.tsv 파일로 저장
  * 단어와 품사 `Tab`으로 구분 
  ```tsv
  # [단어] Tab [품사]
  엔엘피 Tab NNG
  DS TF Tab NNG
  ```
### 형태소 분석기 비교

| 형태소 분석기 | 장점 | 단점 |
| --- | --- | --- |
| Kkma | 분석 품질 좋음, 지원 품사 태그 많음 | 분석 속도 느림, 사용자 사전 불완전 동작 |
| Komoran | 자소 분리된 문작과 오탈자에 강함, 사용자 사전 관리 용이 | 적당한 분석 품질과 속도 |
| Okt | 매우 빠른 처리 속도, 정규화 기능 지원 | 사용자 사전 관리 어려움, 분석 일관성 부족 |

## 임베딩

> 단어나 문장을 수치화해 벡터 공간으로 표현하는 과정<br/>
> 임베딩 품질이 좋다면 단순한 모델로도 훌륭한 결과를 얻을 수 있다.

* 임베딩은 말뭉치의 의미에 따라 벡터화하기 때문에 문법적인 정보가 포함되어 있다.
* 임베딩 기법
  * 문장 임베딩
    * 문장 전체를 벡터로 표현하는 방법
    * 전체 문장의 흐름을 파악해 벡터로 변환하기 때문에 문맥적 의미를 지니는 장점 
    * 장점: 단어 임베딩에 비해 품질이 좋으며, 상용 시스템에 많이 사용 
    * 단점: 많은 문장 데이터 필요, 학습 비용 큼.
  * 단어 임베딩 
    * 개별 단어를 벡터로 표현하는 방법
    * 장점: 학습 방법 간단, 실무 많이 사용 
    * 단점: 동음이의어 구분 안 됨 

### 단어 임베딩

> 단어 임베딩은 말뭉치에서 각각의 단어를 벡터로 변환하는 기법 

#### 원-핫 인코딩(one-hot encoding)

> 단어를 숫자 벡터로 변환하는 가장 기본적인 방법

* 요소들 중 단 하나의 값만 1이고 나머지 요소값은 0인 인코딩
* 원-핫 인코딩으로 나온 결과를 원-핫 벡터
* 원-핫 인코딩을 하기 위해서 단어 집합이라 불리는 사전을 만들어야 함
  * 사전: 말뭉치에서 나오는 서로 다른 단어의 집합 
* 장점: 간단한 구현 방법에 비해 좋은 성능 제공
* 단점: 단어의 의미, 유사한 단어 관계 정보 없음, 단어 사전 크기가 커지면 메모리 낭비와 복잡도 증가(비효율적)

#### 희소 표현과 분산 표현

* 희소 표현
  * 원-핫 인코딩은 희소 벡터(희소 행렬)이고 이런 희소 벡터 표현 방식을 희소 벡터라고 함.
* 분산 표현 
  * 자연어 처리를 잘하기 위해서 기본 토큰이 되는 단어의 의미와 주변 단어 간의 관계가 단어 임베딩에 표현되어야 함.
  * 각 단어 간의 유사성을 잘 표현하면서 벡터 공간을 절약할 수 있는 방법
  * 한 단어의 정보가 여러 차원에 분산되어 표현
  * 분산 표현 방식은 우리가 원하는 차원에 데이터를 최대한 밀집시킬수 있어 밀집 표현(dense representation)이라 부르기도 하며, 밀집 표현으로 만들어진 벡터를 밀집 벡터(dense vector)라고 함.   
  * 장점
    1. 임베팅 벡터의 차원을 데이터 손실을 최소화하면서 압축
    2. 단어의 의미, 주변 단어간의 관계 등 많은 정보가 내포되어 있어 일반화 능력 뛰어남.

#### Word2Vec

> 2013년 구글에서 발표했으며 가장 많이 사용하고 있는 단어 임베딩 모델<br/>
> 계산량을 획기적으로 줄여 빠른 학습 가능<br/>

* Word2Vec 모델은 CBOW(Continuous bag-of-words, 이하 CBOW)와 skip-gram 두 가지 모델 제안
  * CBOW 모델: 주변 단어들을 이용해 타깃 단어를 예측하는 신경망 모델
  * skip-gram 모델: CBOW 모델과 반대로 하나의 타깃 단어를 이용해 주변 단어들을 예측하는 신경망 모델 
* Gensim 패키지 사용: 토픽 모델링과 자연어 처리를 위한 라이브러리
  * 사용법이 간단하고 임베딩 품질이 나쁘지 않아 많이 사용 

## 텍스트 유사도

> 문장을 하나의 벡터로 묶어서 문장간 유사도 계산

### n-gram 유사도

> n-gram은 주어진 문장에서 n개의 연속적인 단어 시퀀스(단어 나열)를 의미<br/>
> n개의 단어를 토큰으로 사용, 이웃한 단어의 출현 횟수를 통계적으로 표현해 텍스트의 유사도를 계산하는 방법<br/>

* 장점: 손쉬운 구현 방식에 비해 학습 말뭉치 품질만 좋다면 괜찮은 성능 제공
* n에 따른 n-gram
  1. n이 1인 경우
     * 1-gram
     * 유니그램
  2. n이 2인 경우
     * 2-gram
     * 바이그램
  3. n이 3인 경우
     * 3-gram
     * 트라이그램
  4. n이 4 이상
     * 숫자만 앞쪽에 붙임
* n-gram 유사도 계산 흐름
  1. 문장을 n-gram으로 토큰을 분리한 후 단어 문서 행렬(Term-Document Matrix, TDM) 생성
  2. 두 문장을 서로 비교해여 동일한 단어의 출현 빈도를 확률로 계산 
* 보통 1~5 사이의 값 많이 사용

### 코사인 유사도

> 단어나 문장을 벡터로 표현할 수 있다면 벡터 간 거리나 각도를 이용해 유사성 파악<br/>
> 두 벡터 간 코사인 각도를 이용해 유사도를 측정하는 방법

* 벡터의 크기와 상관없이 결과가 안정적임
* 다양한 차원에서 적용 가능해 실무에서 많이 사용

## 챗봇 엔진에 필요한 딥러닝 모델

* 케라스: 신경망 모델을 구추할 수 있는 고수준 API 라이브러리

### 인공 신경망(Artificial Neural Network)

> 두뇌의 신경세포인 뉴런을 수학적으로 모방한 모델 

* <img src="https://render.githubusercontent.com/render/math?math=y = (w_0x_0 %2B w_1x_1 %2B w_2x_2) %2B b">

  * 실제 뉴런은 입력된 신호가 특정 강도 이상일 때만 다음 뉴런으로 신호를 전달, 인공 신경망에서는 활성화 함수가 동일한 역할 수행 
    * 활성화 함수: 가중치 계산 결과값 y가 최종적으로 어떤 형태의 출력값으로 보낼지 결정 
  * 활성화 함수 종류
    * 스텝 함수(step function)
      * 그래프 모양이 계단과 같아 스텝 함수라고 함
      * 입력값이 0보다 클 때는 1로, 0 이하일 때는 0으로 반환 
    * 시그모이드 함수(sigmoid function)
      * 0에서 1까지의 출력값이 확률로 표현 
    * ReLU(Rectified Linear Unit)
      * 입력값이 0 이상인 경우에는 기울기가 1인 직선이고, 0보다 작을 땐 결과값 0 반환 
      * 시그모이드 함수에 비해 연산 비용이 크지 않아 학습 속도가 빠름
      * 시그모이드 함수의 문제를 완화시키는 데 효과적이라 많이 사용 
* 입력층과 1개 이상의 은닉층, 출력층으로 구성되어 있는 신경망을 심층 신경망(Deep Neural Network, DNN)이라 함 
* 신경망 계층이 깊게 구성되어 각각의 뉴런을 학습 시킨다 하여 딥러닝 모델이라 함
* 순전파(forward propagation): 신경망 모델에서 입력층으로부터 출력층까지 데이터가 순방향으로 전파되는 과정
* 역전파(back propagation): 목표하는 실제값과 비교해 오차가 많이 발생한다면 다음 순전파 진행 시 오차가 줄어드는 방향으로 가증치를 역방향으로 갱신 
* 성능 좋은 모델: 순전파의 결과값과 실제값의 차이가 크지 않은 모델 

### 문장 분류를 위한 CNN(Convolutional Neural Network) 모델

> CNN은 합성곱 신경망으로 불리며, 컴퓨터 비전 분야에서 대표적으로 사용되는 딥러닝 모델 

#### CNN 모델 개념

> CNN을 이해하려면 합성곱(convolution)과 풀링(pooling)연산이 무엇인지 알아야 함

* 합성곱 연산
  * 합성곱 필터로 불리는 특정 크기의 행렬을 이미지 데이터(혹은 문장 데이터) 행렬에 슬라이딩하면서 곱하고 더하는 연산 
* 풀링 연산
  * 합성곱 연산 결과로 나온 특징맵의 크기를 줄이거나 주요한 특징을 추출하기 위해 사용하는 연산
  * 최대 풀링(max pooling)과 평균 풀링(average pooling) 연산이 있음, 주로 최대 풀링 연산을 사용 

#### 챗봇 문답 데이터 감정 분류 모델 구현

> CNN의 경우 이미지 분류 외에 임베딩 품질만 괜찮다면 자연어 분류에도 좋은 성능을 냄 

1. 필요한 모듈 임포트
2. 데이터 읽어오기
3. 단어 인덱스 시퀀스 벡터 
  * text_to_word_sequence() 함수를 이용해 단어 시퀀스 생성
    * 단어 시퀀스: 단어 토큰들의 순차적 리스트
  * 전체 벡터 크기를 동일하게 맞춰야 함
  * 패딩 처리: MAX_SEQ_LEN 크기보다 작은 벡터에는 남는 공간이 생기는데, 이를 0으로 채우는 작업
4. 학습용, 검증용, 테스트용 데이터셋 생성
5. 하이퍼파라미터 설정
6. CNN 모델 정의
   * 문장을 감정 클래스로 분류하는 CNN 모델은 전처리된 입력 데이터를 단어 임베딩 처리하는 영역과 합성곱 필터와 연산을 통해 문장의 특징 정보(특징맵)를 추출하고, 평탄화(flatten)를 하는 영역, 그리고 완전 연결 계층(fully connected layer)을 통해 감정별로 클래스를 분류하는 영역으로 구성 
7. 3, 4, 5-gram 이후 합치기
8. 모델 생성
9. 모델 학습
10. 모델 평가
11. 모델 저장

### 개체명 인식을 위한 양방향 LSTM 모델

#### RNN(Recurrent Neural Network)

> LSTM은 RNN 모델에서 파생, RNN은 순환 신경망으로 불리며, 앞서 배운 신경망 모델과 다르게 은닉층 노드의 출력값을 출력층과 그 다음 
> 시점의 은닉층 노드의 입력으로 전달해 순환하는 특징을 갖는다.

* RNN 모델에서 x는 입력 벡터 y는 출력 벡터
* t는 현재 시점을 의미
* x_t는 현재 시점의 입력 벡터, y_t는 현재 시점의 출력 벡터를 의미
* 은닉층 노드는 이전 시점(t-1)의 상태값을 저장하는 메모리 역할을 수행하기 때문에 셀(cell) 또는 메모리 셀이라고 함
* 은닉층은 메모리 셀의 벡터는 출력층과 다음 시점(t+1)의 메모리 셀에 전달되는데 이를 은닉 상태(hidden state)라고 함

#### LSTM(Long Short Term Memory)

> RNN 모델은 입력 시퀀스의 시점(time step)이 길어질수록 앞쪽의 데이터가 뒤쪽으로 잘 전달되지 않아 학습 능력이 저하된다.
> 이런 문제를 보완하기 위해 기존 RNN 을 변형해 LSTM 개발

* LSTM은 기본적인 RNN의 은닉 상태를 계산하는 방식에 변화가 있으며, 은닉 상태값 이외에 셀 상태값이 추가됨 
* LSTM은 입력 게이트(Input Gate), 삭제 게이트(Forget Gate), 출력 게이트(Output Gate)로 구성
  * 입력 게이트
    * 현재 정보를 얼마나 기억할지 결정하는 게이트
    * 현재 시점의 입력값(x_t)과 이전 시점의 은닉 상태값(h_t-1)을 연관된 가중치로 곱해 2개의 활성화 함수로 계산 
  * 삭제 게이트
    * 이전 시점의 셀 상태값을 삭제하기 위해 사용
    * 현재 시점의 입력값(x_t)과 이전 시점의 은닉 상태값(h_t-1)을 시그모이드 함수를 통해 0~1 사이의 값으로 출력
  * 출력 게이트
    * 출력 게이트의 결과값(o_t)은 현재 시점의 은닉 상태(h_t)를 결정하는 데 사용되며, 해당 값은 전달되는 메모리 셀이 많아 질수록 정보 유실이 크기 때문에 단기 상태라고 함.
    * 단기 상태(은닉 상태)는 장기 상태(셀 상태값)에 영향을 받는 구조

#### 양방향 LSTM(Bidirectional LSTM)

> 순환 신경망의 구조적 특성상 데이터가 입력 순으로 처리되기 때문에 이전 시점의 정보만 활용할 수밖에 없는 단점이 존재함.
> 문장이 길어질수록 성능이 저하됨.

* 자연어 처리에 있어 입력 데이터의 정방향 처리만큼 역방향 처리도 중요.
* 양방향 LSTM은 기존 LSTM 계층에 역발향으로 처리하는 LSTM 계층을 하나 더 추가해 양방향에서 문장의 패턴을 분석할 수 있도록 구성
* 입력 문장을 양방향에서 처리하므로 시퀀스 길이가 길어진다 하더라도 정보 손실 없이 처리가 가능

#### 개체명 인식(Named Entity Recognition, NER)

> 문장 내에 포함된 어떤 단어가 인물, 장소, 날자 등을 의미하는 단어인지 인식하는 것

* 개체명 인식은 챗봇에서 문장을 정학하게 해석하기 위해 반드시 해야 하는 전처리 과정
* 단순한 질문 형태라면 개체명 사전을 구축해 해당 단어들과 매핑되는 개체명을 찾을 수 있고 이 방법도 현업에서 많이 사용하는 방법이며 상황에 따라 높은 성능 제공
* 문장 구조가 복잡하거나 문맥에 따라 단어의 의미가 바뀐다면 딥러닝 모델 활용해야 함
  * 개체명 사전 구축 방식은 신조어나 사전에 포함되지 않은 단어는 처리 불가능하며 사람이 직접 사전 데이터를 관리해야 함
* 개체명 인식기는 개발하기 까다로운 영역이며 특정 도메인에 특화된 개체명 인식 모델을 개발하기 위해서는 학습 데이터 생성에 많은 시간과 비용이 소요됨 

## 챗봇 학습툴 만들기

> 학습 데이터를 DB에 저장했을 때 실시간으로 챗봇 시스템에 적용될 수 있도록 제작하는 것

* 챗봇 엔진은 크게 2가지 과정을 거쳐서 답변을 출력
  1. 입력되는 문장을 자연어 처리하여 해당 문장의 의도, 개체명, 키워드 정보 등을 추출 
     * 문장입력 -> 챗봇 엔진 -> 엔진 해석 결과(의도, 개체명, 키워드)
  2. 엔진에서 해석한 결과를 이용해 학습 DB 내용을 검색 
     * 답변 출력 -> 챗봇 엔진 -> 학습 DB

### 프로젝트 구조

* chatbot
  * train_tools: 챗봇 학습툴 관련 파일 
  * models: 챗봇 엔진에서 사용하는 딥러닝 모델 관련 파일
    * intent: 의도 분류 모델 관련 파일
    * ner: 개체 인식 모델 관련 파일
  * utils: 챗봇 개발에 필요한 유틸리티 라이브러리
  * config: 챗봇 개발에 필요한 설정
  * test: 챗봇 개발에 필요한 테스트 코드

### 데이터베이스 설계

> 목표하는 챗봇은 간단한 토이 챗봇이므로 데이터 무결성이나 정규화 부분은 크게 신경쓰지 않음

* 테이블 명: chatbot_train_data

| 컬럼 | 속성 | 설명 |
| --- | --- | --- |
| id | int, PK, not null | 학습 데이터 id |
| intent | varchar(45), null | 의도명, 의도가 없는 경우 null |
| ner | varchar(45), null | 개체명, 개체명이 없는 경우 null |
| query | text, null | 질문 텍스트 |
| answer | text, not null | 답변 텍스트 |
| answer_image | varchar(2048), null | 답변에 들어갈 이미지 URL, 없을 경우 null |

* DB 접속 정보
  * config(디렉토리)
    * DatabaseConfig.py
* 챗봇 데이터 학습용 테이블 생성
  * train_tools(디렉토리)
    * qna(디렉토리)
      * create_train_data_table.py

### 챗봇 학습 데이터 엑셀 파일 및 DB 연동

> 학습툴은 화면이 없기 때문에 엑셀을 통해 학습 데이터를 추가하가나 삭제<br/>
> 엑셀 파일을 학습툴에 입력해 DB 내용을 업데이트

* 학습 데이터 엑셀 파일 내용

| 컬럼 | 설명 |
| --- | --- |
| 의도(Intent) | 질문의 의도를 나타내는 텍스트. 의도가 없는 경우 비움 |
| 개체명 인식(NER) | 질뭉에 필요한 개체명. 개체명이 없는 경우 비움 |
| 질문(Query) | 질문 텍스트 |
| 답변(Answer) | 답변 텍스트 |
| 답변 이미지 | 답변에 들어갈 이미지 URL, 이미지 URL 없을 경우 비움 |

## 챗봇 엔진 만들기

> 챗봇에서 핵심 기능을 하는 모듈이며, 화자의 질문을 이해하고 알맞은 답변을 출력하는 역할(자연어 처리 모듈)

### 챗봇 엔진 구조

* 챗봇 엔진 핵심 기능
  1. 질문 의도 분류
     * 화자의 질문 의도를 파악 
     * 질문을 의도 분류 모델을 이용하여 의도 클래스 예측 
  2. 개체명 인식
     * 화자의 질문에서 단어 토큰별 개체명 인식
     * 단어 토큰에 맞는 개체명 예측
  3. 핵심 키워드 추출
     * 화자의 질문 의미에서 핵심이 될 만한 단어 토큰 추출 
     * 형태소 분석기를 이용해 핵심 키워드(명사, 동사) 추출 
  4. 답변 검색
     * 질문의 의도, 개체명, 핵심 키워드를 기반으로 학슴 DB에서 답변 검색
  5. 소켓 서버 
     * 다양한 종류의 챗봇 클라이언트에서 요청하는 질문 처리하기 위한 소켓 서버 프로그램
* 챗봇 엔진 처리 과정
  1. 사용자 질문
  2. 챗봇 엔진
     1. 전처리 과정: 불용어 제거, 토큰 분리
     2. 의도 분석
     3. 개체명 인식
     4. 답변 검색
     5. 답변 출력
  3. 사용자 응답

### 전처리 과정

* 전처리 과정은 자연어 처리에 있어 제일 처음 수행해야 하는 기본적인 과정
* 성능을 좌우하는 매우 중요한 작업
* 전처리 과정
  1. 형태소 분석기로 토크나이징 작업
  2. 문장 해석에 의미 있는 정보만 남기고 나머지 불용어 제거

### 단어 사전 구축 및 시퀀스 생성

* 의도 분류 및 개체명 인식 모델의 학습을 하려면 단어 사전 구축 필요
* 단어 사전 구축을 위해 말뭉치 데이터 필요 
* 챗봇 품질 향상을 위해 주기적인 단어 사전 업데이트 필요 

### 의도 분류 모델

> 전처리 과정을 거친 후 해당 문장의 의도 분류, CNN 모델 이용<br/>
> 다양한 의도를 분류하기엔 학습 데이터 수가 한정적이라 인사, 욕설, 주문, 예챡, 기타 5가지 의도만 분류 

* 챗봇 엔진의 의도 분류 클래스 종류 

| 의도명 | 분류 클래스 | 설명 |
| --- | --- | --- |
| 인사 | 0 | 텍스트가 인사말인 경우 |
| 욕설 | 1 | 텍스트가 욕설인 경우 |
| 주문 | 2 | 텍스트가 주문 관련 내용인 경우 |
| 예약 | 3 | 텍스트가 예약 관련 내용인 경우 |
| 기타 | 4 | 어떤 의도에도 포함되지 않은 경우 |

### 개체명 인식 모델 학습

> 입력된 문장의 의도가 분류된 후 문장 내 개체명 인식(Named Entity Recognition) 진행, 양방향 LSTM 모델 사용

* 개체명 종류

| 개체명 | 설명 |
| --- | --- |
| B_FOOD | 음식 |
| B_DT, B_TI | 날짜, 시간 |
| B_PS | 사람 |
| B_OG | 조직, 회사 |
| B_LC | 지역 |

### 답변 검색

> 해석된 데이터를 기반으로 적절한 답변을 학습 DB로부터 검색, 매우 종요하고 기술적 난이도가 있는 영역 

* 해석된 결과 항목에 따라 학습DB에서 어떤 방식으로 답변을 검색할지 결정은 챗봇 엔진 설계자의 몫

### 챗봇 엔진 서버 개발

> 다양한 플랫폼에서 사용할 수 있도록 서버용 프로그램 제작하는 것

* 다수의 챗봇 서비스가 접속해 화자의 질의에 대한 응답을 제공할 수 있도록 구현 
* JSON 문자열 형태의 통신 많이 사용 
  * JSON(JavaScript Object Notation): Key/Value 쌍으로 이루어진 데이터 객체를 전달하기 위해 인간이 읽을 수 있는 텍스트 형태를 사용하는 개방형 포맷
